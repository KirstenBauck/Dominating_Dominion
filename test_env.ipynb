{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from external.pydominion.dominion import Piles, Phase\n",
    "from external.pydominion.dominion.Game import Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For any testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(\n",
    "    numplayers=2,\n",
    "    initcards=[\"Cellar\", \"Market\", \"Militia\", \"Mine\", \"Moat\", \n",
    "                \"Remodel\", \"Smithy\", \"Village\", \"Throne Room\", \"Workshop\"],\n",
    "    validate_only=False,\n",
    "    prosperity=False,\n",
    "    potions = False,\n",
    "    shelters = False,\n",
    "    card_path=\"external/pydominion/dominion/cards\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.start_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing to look into the options a bit more\n",
    "options = game.current_player._choice_selection()\n",
    "options[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registration/Creation/Playing\n",
    "This all works right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register\n",
    "gym.register(\n",
    "    id=\"Dominion-v14\",\n",
    "    entry_point=\"DominionEnv:DominionEnv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Dominion-v14\", \n",
    "         num_players=2, \n",
    "         card_set=[\"Cellar\", \"Market\", \"Militia\", \"Mine\", \"Moat\", \n",
    "                   \"Remodel\", \"Smithy\", \"Village\", \"Throne Room\", \"Workshop\"],\n",
    "         quiet_flag=True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is what I was using to test when it was still player input and not an agent learning. Don't run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some basic parameters for the loop\n",
    "episode_over = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "\n",
    "# Run the game loop\n",
    "while not episode_over and step_count < 100:  # Limit to 100 steps to prevent infinite loops\n",
    "    step_count += 1\n",
    "\n",
    "    action = env.action_space.sample() \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "# Print end game\n",
    "print(f\"Game finished! Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with an agent?\n",
    "Works... but not really because agent can handle user input I don't think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 703  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013879124 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.76       |\n",
      "|    explained_variance   | -7.185498   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00606    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 0.000155    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011794353 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | -1.0472567  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 3.6e-05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014573618 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | -0.13672423 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 3.6e-05     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009872187  |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.7         |\n",
      "|    explained_variance   | -0.018744111 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0188       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 2.52e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.012136025  |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.68        |\n",
      "|    explained_variance   | -0.052295685 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0018       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00634     |\n",
      "|    value_loss           | 3.68e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008539362  |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.65        |\n",
      "|    explained_variance   | -0.022865057 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0346      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00745     |\n",
      "|    value_loss           | 6.1e-06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 442          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.011063785  |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.64        |\n",
      "|    explained_variance   | -0.030447245 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00638     |\n",
      "|    value_loss           | 1.48e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 438         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00836671  |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.62       |\n",
      "|    explained_variance   | -0.03331995 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    value_loss           | 5.06e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 433          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009718427  |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.6         |\n",
      "|    explained_variance   | -0.066299915 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0181      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00833     |\n",
      "|    value_loss           | 2.21e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 433         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011604879 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | -0.15756297 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00872    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0062     |\n",
      "|    value_loss           | 3.45e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 433         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012698565 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | -0.26106822 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00968    |\n",
      "|    value_loss           | 7.56e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 433         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013997195 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | -0.41977    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0309     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 3.72e-05    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This runs in a minute, do we need to update our timesteps <-- Look through this more closely!\n",
    "from stable_baselines3 import PPO\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"ppo_cartpole\")\n",
    "model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes longer then 45 minutes to run try and figure out what is happening!\n",
    "obs, _ = env.reset()\n",
    "while True:\n",
    "    action, _ = model.predict(obs)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_dominion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
