{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "Change the inputs below\n",
    "- `csv_path`: The path to where `progress.csv` is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"logs/masked_ppo_v5/progress.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "Use the below graphs to see if training went well, can refer to notes on healthy behavior and warning signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Episode Reward & Episode Length Over Time\n",
    "✅ Healthy Behavior:\n",
    "- Reward increases gradually and plateaus — suggests the agent is improving, then stabilizing.\n",
    "- Episode Length may decrease (if the agent is becoming more efficient) or stabilize at a logical average — Dominion is variable-length, so some fluctuation is fine.\n",
    "\n",
    "⚠️ Warning Signs:\n",
    "- Flat reward near zero, agent isn’t learning.\n",
    "- Reward spikes then crashes, unstable training or overfitting.\n",
    "- Episode length increases dramatically, could mean indecision or repetitive loops (like invalid action loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot episode reward and episode length\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df[\"time/total_timesteps\"], df[\"rollout/ep_rew_mean\"], label=\"Mean Episode Reward\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Mean Episode Reward Over Time\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df[\"time/total_timesteps\"], df[\"rollout/ep_len_mean\"], label=\"Mean Episode Length\", color=\"orange\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Episode Length\")\n",
    "plt.title(\"Mean Episode Length Over Time\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Function Loss Over Time\n",
    "✅ Healthy Behavior:\n",
    "- Value loss should decrease steadily and plateau at a low-ish level.\n",
    "- Some fluctuation is normal, especially in more stochastic environments like ours.\n",
    "\n",
    "⚠️ Warning Signs:\n",
    "- Loss increasing or wildly oscillating, value function is unstable or diverging.\n",
    "- Loss stuck high, value net can’t approximate expected returns, maybe due to bad rewards or sparse feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Value function loss\n",
    "plt.plot(df[\"time/total_timesteps\"], df[\"train/value_loss\"], label=\"Loss\")\n",
    "\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Masked PPO Loss over time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Gradient Loss & Approximate KL Divergence\n",
    "✅ Healthy Behavior:\n",
    "- Policy gradient loss doesn’t need to hit zero — some up-and-down is fine.\n",
    "- Approx KL should stay small and stable (PPO wants small policy changes).\n",
    "- A good KL range is ~0.01–0.03. Too high could mean policy is changing too much per update.\n",
    "\n",
    "⚠️ Warning Signs:\n",
    "- KL → 0 and policy loss → 0 = policy is frozen or collapsed (can’t/won’t explore anymore).\n",
    "- KL spikes and loss spikes = unstable or overreactive updates, may need lower learning_rate or smaller clip_range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Policy gradient loss\n",
    "plt.plot(df[\"time/total_timesteps\"], df[\"train/policy_gradient_loss\"], label=\"Policy Gradient Loss\")\n",
    "# Approximate KL divergence\n",
    "plt.plot(df[\"time/total_timesteps\"], df[\"train/approx_kl\"], label=\"Approx. KL Divergence\")\n",
    "\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Loss / KL Value\")\n",
    "plt.title(\"Policy Gradient Loss vs KL Divergence Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy Loss Over Time\n",
    "**Note** A value of -2 has more randomness than -0.1 because this is negative policy entropy\n",
    "\n",
    "✅ Healthy Training Behavior\n",
    "- Starts low (e.g., -2 to -4), agent is exploring actions randomly.\n",
    "- Gradually increases toward 0 over time,  agent is becoming more confident, relying on learned policy.\n",
    "- A smooth curve upward (entropy loss increasing slowly) is a sign of balanced learning \n",
    "\n",
    "⚠️ Warning Signs\n",
    "- Sharp drop then flatlines to 0, policy collapsed too early\n",
    "- Remains very low, still highly random\n",
    "- Oscillations, unstable learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy Loss\n",
    "plt.plot(df[\"time/total_timesteps\"], df[\"train/entropy_loss\"], label=\"Entropy Loss\")\n",
    "\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Entropy Loss\")\n",
    "plt.title(\"Entropy Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
